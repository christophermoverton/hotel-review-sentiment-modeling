{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophermoverton/hotel-review-sentiment-modeling/blob/main/roberta_hotel_review_rating_model_FIXED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEPeuj39F5ZR"
      },
      "source": [
        "\n",
        "# Hotel Review Sentiment Classification Using RoBERTa\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Hotels and hospitality services play a central role in the travel, recreation, and leisure industries. Digital platforms such as TripAdvisor have become essential tools for consumers, providing accessible information, peer evaluations, and rating systems that influence hotel selection decisions. As a result, customer feedback—both in the form of numerical ratings and written reviews—plays a significant role in shaping consumer perceptions and guiding purchasing behavior.\n",
        "\n",
        "Given the importance of user-generated feedback, analyzing the relationship between review text and corresponding ratings offers valuable insight into how consumers evaluate their hotel experiences. Sentiment and rating analysis can help hospitality providers better understand drivers of satisfaction and dissatisfaction, identify operational pain points, and detect mismatches between expressed sentiment and assigned ratings.\n",
        "\n",
        "This study begins by establishing a baseline RoBERTa transformer model to predict hotel ratings (scaled ordinally from 1 to 5) based on review text. We evaluate baseline performance and then introduce class weighting to assess whether correcting for class imbalance (identified during exploratory data analysis) improves model outcomes. Next, we reframe the task into a three-class structure (low, mid, high) to examine whether reducing ordinal granularity enhances separability and predictive stability relative to the baseline. Finally, we analyze transformer-based embeddings within the three-class framework and apply comparative clustering techniques to investigate how sentiment signals align—or diverge—across rating categories.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4iuAY5SVECG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5002aaca"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a70896f"
      },
      "source": [
        "After mounting, specify the path to your file. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "319a1ac3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = '/content/drive/My Drive/tripadvisor_classification_folder/tripadvisor_hotel_reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UyUTSbMXwjd"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0db95da2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Rating', data=df, palette='viridis')\n",
        "plt.title('Distribution of Hotel Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaJMPObKV-zP"
      },
      "source": [
        "There is a fairly sizable class imbalance shown for the different review categories.  Most notably, reviews tend to be predominantly positive, whereas ratings at 3 and below are no morre than 6000 reviews which equals the number of reviews for rating 4 reviews.  Rating 5 predominates with well over 8000 reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz-RY0DvYoiU"
      },
      "outputs": [],
      "source": [
        "df[['Review', 'Rating']].isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhJK-xVPZNlA"
      },
      "source": [
        "No missing values were detected in the Review or Rating columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4_VSS_waBPC"
      },
      "source": [
        "## Modeling Customer Sentiment Review Language for Star Ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCgh1G5iXJjG"
      },
      "source": [
        "\n",
        "\n",
        "### Modeling description\n",
        "\n",
        "1. **Data preparation**\n",
        "\n",
        "   * Starts from a pandas dataframe containing two fields: `Review` (text) and `Rating` (integer 1–5).\n",
        "   * Cleans the data by removing missing/empty reviews and restricting ratings to the valid range (1–5).\n",
        "   * Performs an **80/20 stratified train–validation split** to preserve the rating distribution across splits.\n",
        "\n",
        "2. **Label engineering**\n",
        "\n",
        "   * Converts ratings from **1–5** into **0–4** to match the expected label indexing for cross-entropy classification in Transformers (`labels = Rating - 1`).\n",
        "\n",
        "3. **Tokenization**\n",
        "\n",
        "   * Uses the `roberta-base` tokenizer to convert review text into model inputs (`input_ids`, `attention_mask`).\n",
        "   * Applies truncation to a maximum length of **256 tokens** to control memory and training cost.\n",
        "   * Uses a dynamic padding collator (`DataCollatorWithPadding`) so batches are padded efficiently at runtime.\n",
        "\n",
        "4. **Model**\n",
        "\n",
        "   * Loads `roberta-base` with a **5-class sequence classification head** (`num_labels=5`).\n",
        "   * The model learns to map the review text to one of five rating classes.\n",
        "\n",
        "5. **Training configuration**\n",
        "\n",
        "   * Fine-tunes for **3 epochs** using AdamW-style optimization with:\n",
        "\n",
        "     * learning rate **2e-5**\n",
        "     * weight decay **0.01**\n",
        "     * warmup ratio **0.06**\n",
        "   * Evaluates once per epoch and saves checkpoints per epoch.\n",
        "   * Automatically restores the best checkpoint at the end based on **validation macro-F1**.\n",
        "   * Uses mixed precision (`fp16`) if a CUDA GPU is available.\n",
        "\n",
        "6. **Evaluation**\n",
        "\n",
        "   * Computes:\n",
        "\n",
        "     * **Accuracy**\n",
        "     * **Macro F1** (treats each rating class equally, useful under imbalance)\n",
        "   * After training, generates predictions on the validation set and reports:\n",
        "\n",
        "     * a **confusion matrix** (true vs predicted star ratings)\n",
        "     * a **classification report** (precision/recall/F1 per star rating)\n",
        "\n",
        "In short: this is a **RoBERTa-based 5-class rating prediction model** trained on review text, evaluated with both overall performance metrics and class-level diagnostics to highlight where ordinal confusion occurs (especially in mid-range ratings).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bmt-9b8YzFk"
      },
      "outputs": [],
      "source": [
        "# pip install -q transformers datasets scikit-learn accelerate torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- assumes you already have a pandas df with columns: Review, Rating ---\n",
        "df = df.copy()\n",
        "\n",
        "# Clean\n",
        "df[\"Review\"] = df[\"Review\"].astype(str).str.strip()\n",
        "df = df[df[\"Review\"].notna() & (df[\"Review\"] != \"\") & df[\"Rating\"].notna()]\n",
        "df[\"Rating\"] = df[\"Rating\"].astype(int)\n",
        "df = df[df[\"Rating\"].between(1, 5)]\n",
        "\n",
        "# Train/val split (stratified)\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"Rating\"]\n",
        ")\n",
        "\n",
        "# HF datasets\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "# Map 1–5 -> 0–4 into REQUIRED column name: \"labels\"\n",
        "def add_labels(ex):\n",
        "    ex[\"labels\"] = int(ex[\"Rating\"]) - 1\n",
        "    return ex\n",
        "\n",
        "train_ds = train_ds.map(add_labels)\n",
        "val_ds   = val_ds.map(add_labels)\n",
        "\n",
        "# Tokenize\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok(batch):\n",
        "    return tokenizer(batch[\"Review\"], truncation=True, max_length=256)\n",
        "\n",
        "train_ds = train_ds.map(tok, batched=True)\n",
        "val_ds   = val_ds.map(tok, batched=True)\n",
        "\n",
        "# Keep only what Trainer needs\n",
        "keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep])\n",
        "val_ds   = val_ds.remove_columns([c for c in val_ds.column_names   if c not in keep])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    # works across Trainer API variants\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "# fp16 only if CUDA is available\n",
        "try:\n",
        "    import torch\n",
        "    use_fp16 = torch.cuda.is_available()\n",
        "except Exception:\n",
        "    use_fp16 = False\n",
        "\n",
        "# Training args: support BOTH old and new transformers (eval_strategy vs evaluation_strategy)\n",
        "args_kwargs = dict(\n",
        "    output_dir=\"roberta_reviews_star_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.06,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    logging_steps=50,\n",
        "    fp16=use_fp16,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    args = TrainingArguments(**args_kwargs, eval_strategy=\"epoch\")\n",
        "except TypeError:\n",
        "    args = TrainingArguments(**args_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "# Trainer: DO NOT pass tokenizer (your version doesn't accept it)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate + confusion matrix on val\n",
        "pred = trainer.predict(val_ds)\n",
        "val_logits = pred.predictions\n",
        "val_labels = pred.label_ids\n",
        "val_preds  = np.argmax(val_logits, axis=-1)\n",
        "\n",
        "stars_true = val_labels + 1\n",
        "stars_pred = val_preds + 1\n",
        "\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(stars_true, stars_pred, labels=[1,2,3,4,5]))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(stars_true, stars_pred, labels=[1,2,3,4,5], digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1MkjO9orEq3"
      },
      "source": [
        "F1-scores are strongest for the extreme rating categories (low and high), whereas performance degrades considerably for the mid-range category. This pattern may reflect class imbalance effects, but more likely indicates weaker sentiment segmentation in mid-level reviews, where linguistic signals are inherently more ambiguous and less discriminative.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u06NXZORsGdY"
      },
      "source": [
        "Let's use a class weighting strategy to see if model f1-scores might improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScaqPM5Za5EA"
      },
      "outputs": [],
      "source": [
        "# pip install -q transformers datasets scikit-learn accelerate torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- assumes you already have a pandas df with columns: Review, Rating ---\n",
        "df = df.copy()\n",
        "\n",
        "# Clean\n",
        "df[\"Review\"] = df[\"Review\"].astype(str).str.strip()\n",
        "df = df[df[\"Review\"].notna() & (df[\"Review\"] != \"\") & df[\"Rating\"].notna()]\n",
        "df[\"Rating\"] = df[\"Rating\"].astype(int)\n",
        "df = df[df[\"Rating\"].between(1, 5)]\n",
        "\n",
        "# Train/val split (stratified)\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"Rating\"]\n",
        ")\n",
        "\n",
        "# ---- class weights from TRAIN labels (0..4) ----\n",
        "train_labels_np = (train_df[\"Rating\"].astype(int) - 1).to_numpy()\n",
        "counts = np.bincount(train_labels_np, minlength=5)\n",
        "# inverse-frequency-ish weights; stable + commonly used\n",
        "weights_np = counts.sum() / (len(counts) * np.maximum(counts, 1))\n",
        "# normalize to mean=1 (optional but nice)\n",
        "weights_np = weights_np / weights_np.mean()\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "class_weights = torch.tensor(weights_np, dtype=torch.float)\n",
        "\n",
        "# HF datasets\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "def add_labels(ex):\n",
        "    ex[\"labels\"] = int(ex[\"Rating\"]) - 1\n",
        "    return ex\n",
        "\n",
        "train_ds = train_ds.map(add_labels)\n",
        "val_ds   = val_ds.map(add_labels)\n",
        "\n",
        "# Tokenize\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok(batch):\n",
        "    return tokenizer(batch[\"Review\"], truncation=True, max_length=256)\n",
        "\n",
        "train_ds = train_ds.map(tok, batched=True)\n",
        "val_ds   = val_ds.map(tok, batched=True)\n",
        "\n",
        "# Keep only what Trainer needs\n",
        "keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep])\n",
        "val_ds   = val_ds.remove_columns([c for c in val_ds.column_names   if c not in keep])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "# Custom Trainer with weighted cross-entropy\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# fp16 only if CUDA is available\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "# Training args: supports both eval_strategy and evaluation_strategy\n",
        "args_kwargs = dict(\n",
        "    output_dir=\"roberta_reviews_star_model_weighted\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.06,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    logging_steps=50,\n",
        "    fp16=use_fp16,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    args = TrainingArguments(**args_kwargs, eval_strategy=\"epoch\")\n",
        "except TypeError:\n",
        "    args = TrainingArguments(**args_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate + confusion matrix on val\n",
        "pred = trainer.predict(val_ds)\n",
        "val_logits = pred.predictions\n",
        "val_labels = pred.label_ids\n",
        "val_preds  = np.argmax(val_logits, axis=-1)\n",
        "\n",
        "stars_true = val_labels + 1\n",
        "stars_pred = val_preds + 1\n",
        "\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(stars_true, stars_pred, labels=[1,2,3,4,5]))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(stars_true, stars_pred, labels=[1,2,3,4,5], digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTo1TlReH6wy"
      },
      "source": [
        "The degraded F1 performance in the middle-tier category suggests structural ambiguity rather than simple model deficiency. Mid-range reviews often contain mixed or attenuated sentiment signals, where textual polarity does not cleanly correspond to the assigned ordinal rating. Reviewers may downgrade without explicit negative language or, alternatively, provide critical commentary while still assigning a moderate score. This sentiment–rating miscalibration diminishes class separability and weakens predictive performance for the mid category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Nb2XgqJUkN"
      },
      "source": [
        "We can reframe the task into a three-class rating system that maintains distinct high and low performance categories while collapsing mid-range ratings into a single class. While this approach sacrifices some classification nuance, it reduces ordinal noise and enhances model stability by acknowledging the inherent ambiguity of mid-tier sentiment signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5HJef8lJT8X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "df = df.copy()\n",
        "\n",
        "df[\"Review\"] = df[\"Review\"].astype(str).str.strip()\n",
        "df = df[df[\"Review\"].notna() & (df[\"Review\"] != \"\") & df[\"Rating\"].notna()]\n",
        "df[\"Rating\"] = df[\"Rating\"].astype(int)\n",
        "df = df[df[\"Rating\"].between(1,5)]\n",
        "\n",
        "# 3-class mapping: Low(1-2)= 0, mid(3) = 1, high(4-5) = 2\n",
        "\n",
        "def to_3class(r):\n",
        "  if r <= 2:\n",
        "    return 0\n",
        "  elif r == 3:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "\n",
        "df[\"y3\"] = df[\"Rating\"].apply(to_3class).astype(int)\n",
        "\n",
        "#stratified split on the new label\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"y3\"]\n",
        ")\n",
        "\n",
        "# hf datasets\n",
        "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "def add_labels(ex):\n",
        "  ex[\"labels\"] = int(ex[\"y3\"])\n",
        "  return ex\n",
        "\n",
        "train_ds = train_ds.map(add_labels)\n",
        "val_ds   = val_ds.map(add_labels)\n",
        "\n",
        "#tokenize\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tok(batch):\n",
        "  return tokenizer(batch[\"Review\"], truncation=True, max_length=256)\n",
        "\n",
        "train_ds = train_ds.map(tok, batched=True)\n",
        "val_ds = val_ds.map(tok, batched=True)\n",
        "\n",
        "#keep only trainer columns\n",
        "keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep])\n",
        "val_ds  = val_ds.remove_columns([c for c in val_ds.column_names if c not in keep])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "#Model: 3 classes\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "# Metrics (macro-F1 is key with imbalance)\n",
        "def compute_metrics(eval_pred):\n",
        "  logits = eval_pred.predictions\n",
        "  labels = eval_pred.label_ids\n",
        "  preds = np.argmax(logits, axis=-1)\n",
        "  return {\n",
        "      \"accuracy\": accuracy_score(labels, preds),\n",
        "      \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
        "  }\n",
        "\n",
        "#fp16 only if CUDA is available\n",
        "import torch\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "# TrainingArguments: handle new vs old API\n",
        "args_kwargs = dict(\n",
        "    output_dir=\"roberta_reviews_3class\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.06,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    logging_steps=50,\n",
        "    fp16=use_fp16,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "try:\n",
        "  args = TrainingArguments(**args_kwargs, eval_strategy=\"epoch\")\n",
        "except TypeError:\n",
        "  args = TrainingArguments(**args_kwargs, evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "#evaluate + confusion matrix\n",
        "pred = trainer.predict(val_ds)\n",
        "logits = pred.predictions\n",
        "y_true = pred.label_ids\n",
        "y_pred = np.argmax(logits, axis=-1)\n",
        "\n",
        "# Pretty labels for reporting\n",
        "label_names = [\"low(1-2)\", \"mid(3)\", \"high(4-5)\"]\n",
        "\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(y_true, y_pred, labels=[0,1,2]))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtb9sizKTR2b"
      },
      "source": [
        "\n",
        "\n",
        "#### Observations\n",
        "\n",
        "**High class (4–5):**\n",
        "F1 ≈ 0.95, demonstrating very strong separability with minimal catastrophic errors (very few high → low misclassifications).\n",
        "\n",
        "**Low class (1–2):**\n",
        "F1 ≈ 0.84, also indicating a strong and relatively well-defined sentiment signal.\n",
        "\n",
        "**Mid class (3):**\n",
        "F1 ≈ 0.50, which is expected given the linguistic ambiguity inherent to three-star ratings.\n",
        "\n",
        "The weaker performance in the mid class does not necessarily reflect model deficiency; rather, it appears to capture the underlying subjectivity embedded within the rating system itself. A “middle” rating is inherently relative—what constitutes a neutral experience for one reviewer may be perceived as mildly positive or mildly negative by another.\n",
        "\n",
        "From a sentiment perspective, consumers who feel clearly satisfied are more likely to assign higher ratings, while clearly dissatisfied consumers gravitate toward lower ratings. The mid category, however, often reflects mixed or attenuated sentiment—reviews that contain both positive and negative elements without a dominant polarity. The ambiguity arises not from a lack of signal, but from competing signals within the same review, making class boundaries less distinct and inherently more difficult to model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiNqR7K5WnEF"
      },
      "source": [
        "### Extract RoBERTa Embeddings\n",
        "\n",
        "Let's grab some consumer insights from our ratings model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "265a6789"
      },
      "source": [
        "### Save the model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "332d887c"
      },
      "outputs": [],
      "source": [
        "# Define the directory to save the model\n",
        "save_directory = \"./my_fine_tuned_roberta_model\"\n",
        "\n",
        "# Save the model and tokenizer using the trainer\n",
        "trainer.save_model(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb38b17"
      },
      "source": [
        "### Load the saved model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34f55d24"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Define the directory where the model was saved\n",
        "loaded_model_directory = \"./my_fine_tuned_roberta_model\"\n",
        "\n",
        "# Load the tokenizer\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(loaded_model_directory)\n",
        "\n",
        "# Load the model\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(loaded_model_directory)\n",
        "\n",
        "# Move the loaded model to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model.to(device)\n",
        "#loaded_model.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3970d59"
      },
      "source": [
        "### Example of using the loaded model for prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GGdXrI3YwT0"
      },
      "source": [
        "#### Cluster Within Each Rating Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptt882OSUzZ4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "def get_embeddings(texts, batch_size=32):\n",
        "  embeddings = []\n",
        "  for i in tqdm(range(0, len(texts), batch_size)):\n",
        "    batch = texts[i:i+batch_size]\n",
        "    inputs = loaded_tokenizer(\n",
        "        batch,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Access the base RoBERTa model to get its outputs including last_hidden_state\n",
        "      outputs = loaded_model.roberta(**inputs)\n",
        "      cls_emb = outputs.last_hidden_state[:, 0, :]  #cls token\n",
        "      embeddings.append(cls_emb.cpu())\n",
        "\n",
        "  return torch.cat(embeddings)\n",
        "\n",
        "embeddings = get_embeddings(df[\"Review\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKdsmVeBY0gY"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "df[\"emb_idx\"] = range(len(df))\n",
        "\n",
        "cluster_results = {}\n",
        "\n",
        "for class_id, class_name in zip([0,1,2], {\"low\",\"mid\",\"high\"}):\n",
        "  mask = df[\"y3\"] == class_id\n",
        "  class_emb = embeddings[mask]\n",
        "\n",
        "  kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "  clusters = kmeans.fit_predict(class_emb)\n",
        "\n",
        "  df.loc[mask, \"cluster\"] = clusters\n",
        "  cluster_results[class_name] = kmeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsDpSC86eipK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def get_cluster_keywords(class_id, cluster_id, top_n=15):\n",
        "    subset = df[(df[\"y3\"] == class_id) & (df[\"cluster\"] == cluster_id)]\n",
        "\n",
        "    # Handle case where subset is empty to avoid ValueError\n",
        "    if subset.empty:\n",
        "        print(f\"Warning: No reviews found for class_id={class_id}, cluster_id={cluster_id}\")\n",
        "        return []\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_features=3000,\n",
        "        ngram_range=(1,2),\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    X = vectorizer.fit_transform(subset[\"Review\"])\n",
        "    scores = np.asarray(X.mean(axis=0)).flatten()\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    if len(feature_names) == 0:\n",
        "        print(f\"Warning: No keywords found for class_id={class_id}, cluster_id={cluster_id} after TF-IDF.\")\n",
        "        return []\n",
        "\n",
        "    top_idx = scores.argsort()[-top_n:][::-1]\n",
        "    return feature_names[top_idx]\n",
        "\n",
        "# Example: print keywords for high bucket clusters\n",
        "# Iterate only up to the actual number of clusters (3 in this case)\n",
        "for cluster_id in range(3):\n",
        "    print(f\"\\nHigh Cluster {cluster_id}\")\n",
        "    print(get_cluster_keywords(2, cluster_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We65X3kygsLA"
      },
      "outputs": [],
      "source": [
        "bucket_map = {\n",
        "    0: \"low (1–2)\",\n",
        "    1: \"mid (3)\",\n",
        "    2: \"high (4–5)\"\n",
        "}\n",
        "\n",
        "for class_id, name in bucket_map.items():\n",
        "    print(f\"\\n\\n===== {name.upper()} =====\")\n",
        "    for cluster_id in range(5):\n",
        "        print(f\"\\nCluster {cluster_id}\")\n",
        "        print(get_cluster_keywords(class_id, cluster_id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4g9eBSUtIz2"
      },
      "source": [
        "We observe that while clusters appear generally positive, they lack meaningful segmentation. There is substantial vocabulary overlap across customer rating categories, which weakens interpretability and reduces the distinctiveness of each cluster. The current approach highlights frequently occurring terms, but not necessarily the terms that differentiate one cluster from another.\n",
        "\n",
        "To address this, we introduce an additional method focused on cluster distinctiveness. Specifically, we compute the mean vector representation of the entire rating bucket and compare it to the mean vector representation of each cluster within that bucket. By differencing the cluster mean from the bucket mean, we obtain a distinctiveness score that highlights features disproportionately represented in a given cluster relative to its broader class context. Sorting these scores allows us to extract the most distinctive and representative terms for each cluster.\n",
        "\n",
        "This approach shifts the objective from identifying common vocabulary to identifying discriminative vocabulary, thereby improving semantic segmentation and interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4ht6TUbhp_8"
      },
      "outputs": [],
      "source": [
        "def get_distinctive_cluster_keywords(class_id, cluster_id, top_n=15):\n",
        "    # Subset cluster\n",
        "    cluster_subset = df[(df[\"y3\"] == class_id) & (df[\"cluster\"] == cluster_id)]\n",
        "\n",
        "    if len(cluster_subset) == 0:\n",
        "        return []\n",
        "\n",
        "    # All reviews in same bucket\n",
        "    bucket_subset = df[df[\"y3\"] == class_id]\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_features=4000,\n",
        "        ngram_range=(1,2),\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    # Fit on bucket\n",
        "    X_bucket = vectorizer.fit_transform(bucket_subset[\"Review\"])\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Transform cluster subset using same vocab\n",
        "    X_cluster = vectorizer.transform(cluster_subset[\"Review\"])\n",
        "\n",
        "    # Mean scores\n",
        "    bucket_mean = np.asarray(X_bucket.mean(axis=0)).flatten()\n",
        "    cluster_mean = np.asarray(X_cluster.mean(axis=0)).flatten()\n",
        "\n",
        "    # Distinctiveness score\n",
        "    distinctive_score = cluster_mean - bucket_mean\n",
        "\n",
        "    top_idx = distinctive_score.argsort()[-top_n:][::-1]\n",
        "    return feature_names[top_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD_LuHochtE4"
      },
      "outputs": [],
      "source": [
        "for cluster_id in range(3):  # you only have clusters 0-2 active\n",
        "    print(f\"\\nLow Cluster {cluster_id}\")\n",
        "    print(get_distinctive_cluster_keywords(0, cluster_id))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PEpc1DljR_v"
      },
      "source": [
        "The Bucket–Cluster Contrast analysis reveals a sharp distinction between clearly negative reviews and reviews that merely result in lower ratings without strongly negative language. Among the keywords associated with lower-rating candidates, we observe terms such as “ok,” “nice,” “average,” “little,” “small,” and “location,” which suggest mild dissatisfaction, unmet expectations, or contextual trade-offs (e.g., walkability or room size) rather than overt negativity.\n",
        "\n",
        "In contrast, the distinctly negative cluster contains much stronger polarity markers, including “dirty,” “worst,” “terrible,” “horrible,” “rude,” “avoid,” “bad,” and “manager.” Notably, the term “experience” appears in a negative framing here and does not surface prominently within the positive-rating vocabulary. These terms signal clear dissatisfaction and emotional intensity, indicating that this cluster captures reviews with unambiguous negative sentiment rather than mild or mixed evaluations.\n",
        "\n",
        "This contrast demonstrates that not all low ratings are linguistically equivalent—some reflect moderate disappointment, while others express strongly articulated negative experiences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ECk4_rti0xn"
      },
      "outputs": [],
      "source": [
        "for cluster_id in range(3):  # you only have clusters 0-2 active\n",
        "    print(f\"\\nMid Cluster {cluster_id}\")\n",
        "    print(get_distinctive_cluster_keywords(1, cluster_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7fbLE3bkkjF"
      },
      "source": [
        "\n",
        "\n",
        "For the middle-rating classification (the minority class), we observe three distinct semantic clusters.\n",
        "\n",
        "The first cluster contains terms such as *“good,” “value,” “good value,” “walking distance,”* and *“staff helpful.”* This language suggests pragmatic satisfaction—reviews that acknowledge positive attributes, particularly in relation to cost and convenience, but without strong emotional endorsement.\n",
        "\n",
        "The second cluster includes terms such as *“ok,” “average,” “average hotel,” “decent,” “pretty,”* and *“basic.”* These descriptors reflect neutral or tempered sentiment, often signaling adequacy rather than enthusiasm. The language is evaluative but restrained, aligning with a middle-tier assessment.\n",
        "\n",
        "The third cluster contains stronger negative indicators, including *“smelled,” “poor,” “dirty,” “old,” “disappointed,”* and *“manager.”* This cluster appears to represent a complaint-driven consumer segment. Unlike the first two clusters, which reflect value-oriented or expectation-aligned evaluations, this segment exhibits clearer dissatisfaction signals despite remaining in the mid-rating category.\n",
        "\n",
        "The first two clusters suggest a budget-aware framing of hospitality—reviews where consumers recognize the level of service provided as functional or appropriate for the price point, but not luxurious or exceptional. As such, they do not elevate the stay into higher rating classifications. In contrast, the complaint-driven cluster introduces additional noise into the mid category, as similar dissatisfaction language also appears in low-rated reviews. This overlap likely contributes to reduced separability and weaker F1 performance for the mid class.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huJAvWVajAmx"
      },
      "outputs": [],
      "source": [
        "for cluster_id in range(3):  # you only have clusters 0-2 active\n",
        "    print(f\"\\nHigh Cluster {cluster_id}\")\n",
        "    print(get_distinctive_cluster_keywords(2, cluster_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-eqNXQmiXO"
      },
      "source": [
        "\n",
        "\n",
        "Cluster 0 is readily identifiable as a distinctly positive consumer segment, characterized by strong affective descriptors such as *“wonderful,” “great,” “loved,”* and *“fantastic.”* The concentration of emphatic positive language clearly aligns with genuinely high-rated experiences and reflects unambiguous customer satisfaction.\n",
        "\n",
        "Clusters 1 and 2, however, appear to reflect more tempered or context-dependent sentiment. These clusters include terms such as *“average,” “nice,” “good value,” “good location,” “clean,” “pretty,” “little,”* and *“small.”* This vocabulary suggests a consumer segment that is positively evaluating budget-oriented stays—acknowledging adequacy, value, and acceptable quality rather than excellence. The sentiment is favorable but restrained, indicating satisfaction within the context of price and expectations rather than luxury or exceptional service.\n",
        "\n",
        "Although the term *“bad”* appears in these clusters, it does not co-occur strongly with other high-intensity negative markers, suggesting that its usage may reflect isolated or minor complaints rather than pervasive dissatisfaction. This may indicate cases where a customer experienced a largely positive stay but encountered a specific issue that prompted a slight downgrade in rating—for example, from a potential five-star evaluation to four stars. Such cases illustrate how localized criticism can coexist with overall positive sentiment, reinforcing the nuanced distinction between emphatic praise and qualified approval.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f60d266e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = loaded_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Map 0, 1, 2 back to your original 3-class mapping if applicable\n",
        "    # (e.g., 0 for low, 1 for mid, 2 for high)\n",
        "    # If using the 5-star model, map 0-4 to 1-5 stars.\n",
        "    # Assuming the 3-class model from the last executed cell:\n",
        "    class_names = [\"low (1-2 stars)\", \"mid (3 stars)\", \"high (4-5 stars)\"]\n",
        "    predicted_class = class_names[prediction]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Test with a new review\n",
        "review1 = \"The hotel was absolutely fantastic! Clean rooms, great service, and excellent amenities. Highly recommend!\"\n",
        "review2 = \"It was okay. Nothing special, but not bad either. The breakfast was a bit cold.\"\n",
        "review3 = \"Terrible experience. The room was dirty, and the staff was rude. Will never stay here again.\"\n",
        "\n",
        "print(f\"Review 1: '{review1}' -> Predicted sentiment: {predict_sentiment(review1)}\")\n",
        "print(f\"Review 2: '{review2}' -> Predicted sentiment: {predict_sentiment(review2)}\")\n",
        "print(f\"Review 3: '{review3}' -> Predicted sentiment: {predict_sentiment(review3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gm3iS_EBlr4"
      },
      "source": [
        "# Summary and Conclusion\n",
        "\n",
        "---\n",
        "\n",
        "## Comparative Modeling Results\n",
        "\n",
        "| Approach                         | Label Structure              | Accuracy          | Macro F1   | Key Strengths                                               | Primary Limitation                              |\n",
        "| -------------------------------- | ---------------------------- | ----------------- | ---------- | ----------------------------------------------------------- | ----------------------------------------------- |\n",
        "| **5-Class Ordinal (1–5)**        | Full ordinal granularity     | ~0.68             | ~0.63      | Preserves rating nuance; strong performance at extremes     | Heavy confusion in 2–3–4 due to ordinal overlap |\n",
        "| **5-Class (Weighted Loss)**      | Full ordinal + class weights | ~0.68             | ~0.62–0.63 | Attempts imbalance correction                               | No meaningful improvement; ambiguity dominates  |\n",
        "| **3-Class (Low / Mid / High)**   | 1–2 / 3 / 4–5                | ~0.88             | ~0.76      | Strong separability at extremes; improved stability         | Mid class remains ambiguous (F1 ~0.50)          |\n",
        "| **Embedding-Based Segmentation** | Within-class clustering      | N/A (exploratory) | N/A        | Reveals latent consumer segments; improves interpretability | Not a predictive model; interpretive layer      |\n",
        "\n",
        "---\n",
        "\n",
        "## Class-Level Breakdown (3-Class Model)\n",
        "\n",
        "| Class | Mapping | F1 Score | Interpretation                                          |\n",
        "| ----- | ------- | -------- | ------------------------------------------------------- |\n",
        "| Low   | 1–2     | ~0.84    | Strong negative polarity; clear dissatisfaction signal  |\n",
        "| Mid   | 3       | ~0.50    | Linguistically ambiguous; mixed or attenuated sentiment |\n",
        "| High  | 4–5     | ~0.95    | Strong positive polarity; highly separable              |\n",
        "\n",
        "---\n",
        "\n",
        "## High-Level Takeaways\n",
        "\n",
        "* Extreme ratings are highly predictable due to strong lexical polarity.\n",
        "* Mid-tier ratings suffer from structural ambiguity rather than model weakness.\n",
        "* Class weighting did not materially improve performance, suggesting imbalance was secondary to ordinal noise.\n",
        "* Collapsing to a 3-class framework substantially improved macro F1 by aligning labels with linguistic signal strength.\n",
        "* Embedding-based clustering revealed meaningful semantic subsegments within each rating tier.\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "This analysis demonstrates that sentiment classification performance is strongly influenced by label structure and underlying linguistic separability.  The initial five-class ordinal framework produced moderate performance, particularly struggling with middle-tier ratings due to substantial semantic overlap and ambiguous sentiment expression.\n",
        "\n",
        "Reframing the problem into a three-class system - low (1-2), mid(3), and high(4-5) - substantially improved macro F1 performance by aligning the classification task with clearer sentiment boundaries.  Extreme ratings (low and high) exhibit strong lexical polarity and distinct emotional intensity, resulting in high separabiblity and robust model performance.  In contrast, the mid-rating category reflects inherent human subjectivity where mixed or attenuated sentiment signals reduce class clarity and weaken predictive stability.  \n",
        "\n",
        "Embedding-based clustering within rating buckets further revealed that ratings are not monolithic categories but contain meaningful subsegments.  In the low bucket, we observed separation between severe dissatisfaction and mild disappointment.  In the mid bucket, clusters reflected value-oriented pragmatism, neutral adequacy, and complaint-driven subsegments.  In the high bucket, clusters differentiated emphatic praise from qualified approval, particularly among budget-cconscious consumers.\n",
        "\n",
        "The Bucket-Cluster Contrast method improved interpretability by isolating distinctive lexical features rather than merely frequent terms.  This shift from frequency-based to contrast-based feature extraction clarified semantic segmentation and exposed consumer experience archetypes.  \n",
        "\n",
        "Overall, the findings suggest that model limitations in the mid-tier class stem less from algorithmic weakness and more from the inherent ambiguity of human rating beahvior.  By restructuring the label space and incorporating embedding-based segmentation, we achieved improved performance, stronger interpretability, and a more realistic representation of consumer sentiment dynamics.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpGqGtksVt-z"
      },
      "source": [
        "# Reference\n",
        "\n",
        "Alam, M. H., et al. “Joint Multi-Grain Topic Sentiment: Modeling Semantic Aspects for Online Reviews.” Information Sciences, vol. 339, 2016, pp. 206–223. DOI: [DOI]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h048XxqPbt20"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/roberta_hotel_review_rating_model_ver2.ipynb\"\n",
        "\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove notebook-level widget metadata (if present anywhere)\n",
        "nb.get(\"metadata\", {}).pop(\"widgets\", None)\n",
        "\n",
        "# Strip problematic widget MIME + (optionally) all outputs\n",
        "for cell in nb.get(\"cells\", []):\n",
        "    # safest for GitHub: remove outputs entirely\n",
        "    if cell.get(\"cell_type\") == \"code\":\n",
        "        cell[\"outputs\"] = []\n",
        "        cell[\"execution_count\"] = None\n",
        "\n",
        "    # also remove any lingering widget metadata at cell level\n",
        "    cell.get(\"metadata\", {}).pop(\"widgets\", None)\n",
        "\n",
        "# Optional: also remove any top-level \"widgets\" keys nested elsewhere (rare)\n",
        "def deep_pop_widgets(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        obj.pop(\"widgets\", None)\n",
        "        for v in obj.values():\n",
        "            deep_pop_widgets(v)\n",
        "    elif isinstance(obj, list):\n",
        "        for v in obj:\n",
        "            deep_pop_widgets(v)\n",
        "\n",
        "deep_pop_widgets(nb)\n",
        "\n",
        "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=1)\n",
        "\n",
        "print(\"Stripped outputs + widget remnants. Re-upload/commit this file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7fk9V6Xb0y6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNk5SXTdLcAkP45ja5cnnLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}